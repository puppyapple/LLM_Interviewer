{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puppyapple/anaconda3/envs/bigmodel/lib/python3.10/site-packages/haystack/core/errors.py:34: DeprecationWarning: PipelineMaxLoops is deprecated and will be remove in version '2.7.0'; use PipelineMaxComponentRuns instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "from AIInterviewer.resume_parser import ResumeParser\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from AIInterviewer.knowledge_base import KnowledgeBase\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = ResumeParser(api_type=\"qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projects = rp.parse(\"cv.pdf\")\n",
    "# display(Markdown(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open(\"../AIInterviewer/stopwords.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(\n",
    "    api_type=\"qwen\",\n",
    "    file_dir=\"../AIInterviewer/docs\",\n",
    "    save_dir=\"../AIInterviewer/index\",\n",
    "    stop_words=stop_words,\n",
    "    similarity_cutoff=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in kb.query(\"长期记忆\").source_nodes:\n",
    "    display_source_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AIInterviewer.text_chunker import split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader, PyMuPDFReader\n",
    "\n",
    "reader = PyMuPDFReader()\n",
    "test_md = reader.load_data(\n",
    "    \"../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='8bedce39-dec0-4d93-b4bc-126d9911f11e', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 1\\n1.背景结构\\n1.1 基础知识\\nBERT（Bidirectional Encoder Representations from Transformers）是⾕歌提\\n出，作为⼀个 Word2Vec 的替代者，其在 NLP 领域的 11 个⽅向⼤幅刷新了精度，可\\n以说是近年来⾃残差⽹络最优突破性的⼀项技术了。论⽂的主要特点以下⼏点：\\n使⽤了双向 Transformer 作为算法的主要框架，之前的模型是从左向右输⼊⼀个\\n⽂本序列，或者将 left-to-right 和 right-to-left 的训练结合起来，实验的结果\\n表明，双向训练的语⾔模型对语境的理解会⽐单向的语⾔模型更深刻；\\n使⽤了 Mask Language Model(MLM) 和 Next Sentence Prediction(NSP) 的多\\n任务训练⽬标；\\n使⽤更强⼤的机器训练更⼤规模的数据，使 BERT 的结果达到了全新的⾼度，并\\n且 Google 开源了 BERT 模型，⽤户可以直接使⽤ BERT 作为 Word2Vec 的转换\\n矩阵并⾼效的将其应⽤到⾃⼰的任务中。\\nBERT 只利⽤了 Transformer 的 encoder 部分。因为 BERT 的⽬标是⽣成语⾔模\\n型，所以只需要 encoder 机制。\\n1.2 BERT 与其他模型相⽐\\nRNN/LSTM：可以做到并发执⾏，同时提取词在句⼦中的关系特征，并且能在多\\n个不同层次提取关系特征，进⽽更全⾯反映句⼦语义  \\nword2vec：其⼜能根据句⼦上下⽂获取词义，从⽽避免歧义出现。  \\nELMO：elmo 是伪双向，只是将左到右，右到左的信息加起来，⽽且⽤的是\\nlstm ，同时缺点也是显⽽易⻅的，模型参数太多，⽽且模型太⼤，少量数据训\\n练时，容易过拟合。\\n其次 bert 在多⽅⾯的 nlp 任务表现来看效果都较好，具备较强的泛化能⼒，对于特定\\n的任务只需要添加⼀个输出层来进⾏ fine-tuning 即可\\nbert 细节\\n1.\\n2.\\n3.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='093b618d-a57d-476e-97ad-e6fffe643383', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 2\\n1.3 BERT，GPT，ELMo\\nBERT, GPT, ELMo 之间的不同点\\n关于特征提取器:\\nELMo 采⽤两部分双层双向 LSTM 进⾏特征提取，然后再进⾏特征拼接来融合\\n语义信息。\\nGPT 和 BERT 采⽤ Transformer 进⾏特征提取。BERT 采⽤的是\\nTransformer 架构中的 Encoder 模块；GPT 采⽤的是 Transformer 架构中的\\nDecoder 模块.\\n很多 NLP 任务表明 Transformer 的特征提取能⼒强于 LSTM, 对于 ELMo ⽽⾔, \\n采⽤ 1 层静态 token embedding + 2 层 LSTM，提取特征的能⼒有限。\\n单/双向语⾔模型:\\n三者之中, 只有 GPT 采⽤单向语⾔模型, ⽽ ELMo 和 BERT 都采⽤双向语⾔模\\n型.\\nELMo 虽然被认为采⽤了双向语⾔模型，但实际上是左右两个单向语⾔模型分别\\n提取特征，然后进⾏特征拼接， 这种融合特征的能⼒⽐ BERT ⼀体化的融合特\\n征⽅式弱。\\n三者之中, 只有 ELMo 没有采⽤ Transformer。GPT 和 BERT 都源于\\nTransformer 架构，GPT 的单向语⾔模型采⽤了经过修改后的 Decoder 模块，\\nDecoder 采⽤了 look-ahead mask，只能看到 context before 上⽂信息，未来\\n的信息都被 mask 掉了。⽽ BERT 的双向语⾔模型采⽤了 Encoder 模块，\\nEncoder 只采⽤了 padding mask，可以同时看到 context before 上⽂信息, 以\\n及 context after 下⽂信息。 \\nBERT, GPT, ELMo 各⾃的优点和缺点\\nELMo\\n优点：从早期的 Word2Vec 预训练模型的最⼤缺点出发, 进⾏改进, 这⼀缺点就\\n是⽆法解决多义词的问题。ELMo 根据上下⽂动态调整 word embedding，可以\\n解决多义词的问题。\\n缺点：ELMo 使⽤ LSTM 提取特征的能⼒弱于 Transformer；ELMo 使⽤向量拼\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7ce39022-accf-4415-a026-fc94755fc05e', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '3'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 3\\n接的⽅式融合上下⽂特征的能⼒弱于 Transformer.\\nGPT\\n优点：GPT 使⽤了 Transformer 提取特征, 使得模型能⼒⼤幅提升.\\n缺点：GPT 只使⽤了单向 Decoder，⽆法融合未来的信息.\\nBERT\\n优点：BERT 使⽤了双向 Transformer 提取特征，使得模型能⼒⼤幅提升；添加\\n了两个预训练任务, MLM + NSP 的多任务⽅式进⾏模型预训练.\\n缺点：模型过于庞⼤, 参数量太多, 需要的数据和算⼒要求过⾼, 训练好的模型应\\n⽤场景要求⾼；更适合⽤于语⾔嵌⼊表达，语⾔理解⽅⾯的任务，不适合⽤于⽣\\n成式的任务。\\n1.4 与 Transformer 区别\\n只是使⽤了 transformer 的 encoder  \\n与 Transformer 本身的 Encoder 端相⽐，BERT 的 Transformer Encoder 端输⼊的\\n向量表示，多了 Segment Embeddings。  \\n⽹络层数 L，隐藏层维度 H，Attention 多头个数 A  \\nbase：L=12, H=768, A=12, 110M,使⽤ GPU 内存：7G 多  \\nlarge: L=24,H=1024,A=16, 340M,使⽤ GPU 内存：32G 多  \\ntransformer 是 512 维，encoder 是 6 个堆叠，8 个头，  \\nbert 是 12 个 transformer 叠加。每⼀个 transformer 由 6 个 encoder 叠加\\n1.5 word2vec 到 BERT 改进了什么\\nword2vec 到 BERT 的改进之处其实没有很明确的答案，BERT 的思想其实很⼤程度\\n上来源于 CBOW 模型，如果从准确率上说改进的话，BERT 利⽤更深的模型，以及\\n海量的语料，得到的 embedding 表示，来做下游任务时的准确率是要⽐ word2vec\\n⾼不少的。实际上，这也离不开模型的“加码”以及数据的“巨⼤加码”。再从⽅法的意\\n义⻆度来说，BERT 的重要意义在于给⼤量的 NLP 任务提供了⼀个泛化能⼒很强的\\n预训练模型，⽽仅仅使⽤ word2vec 产⽣的词向量表示，不仅能够完成的任务⽐\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1093d97a-3014-4ae4-8f4c-f7575c797733', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '4'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 4\\nBERT 少了很多，⽽且很多时候直接利⽤ word2vec 产⽣的词向量表示给下游任务提\\n供信息，下游任务的表现不⼀定会很好，甚⾄会⽐较差。\\n2.模型结构\\n2.1 两个任务\\n（1）Masked LM (MLM) \\n在将单词序列输⼊给 BERT 之前，每个序列中有 15％ 的单词被 [MASK]  token 替\\n换。 然后模型尝试基于序列中其他未被 mask 的单词的上下⽂来预测被掩盖的原单\\n词。在 BERT 的实验中，15% 的 WordPiece Token 会被随机 Mask 掉。在训练模型\\n时，⼀个句⼦会被多次喂到模型中⽤于参数学习，但是 Google 并没有在每次都\\nmask 掉这些单词，⽽是在确定要 Mask 掉的单词之后，80% 的概率会直接替换为\\n[Mask] ，10% 的概率将其替换为其它任意单词，10% 的概率会保留原始 Token。\\n80% 的 tokens 会被替换为 [MASK] token：是 Masked LM 中的主要部分，\\n可以在不泄露 label 的情况下融合真双向语义信息；\\n10% 的 tokens 会称替换为随机的 token ：因为需要在最后⼀层随机替换的这\\n个 token 位去预测它真实的词，⽽模型并不知道这个 token 位是被随机替换\\n的，就迫使模型尽量在每⼀个词上都学习到⼀个 全局语境下的表征，因⽽也能够\\n让 BERT 获得更好的语境相关的词向量（这正是解决⼀词多义的最重要特性）；\\n10% 的 tokens 会保持不变但需要被预测 ：这样能够给模型⼀定的 bias ，相当\\n于是额外的奖励，将模型对于词的表征能够拉向词的 真实表征\\n（2）Next Sentence Prediction (NSP) \\n在 BERT 的训练过程中，模型接收成对的句⼦作为输⼊，并且预测其中第⼆个句⼦\\n是否在原始⽂档中也是后续句⼦。\\n在训练期间，50％ 的输⼊对在原始⽂档中是前后关系，另外 50％ 中是从语料库\\n中随机组成的，并且是与第⼀句断开的。\\n在第⼀个句⼦的开头插⼊ [CLS]  标记，表示该特征⽤于分类模型，对⾮分类模\\n型，该符号可以省去，在每个句⼦的末尾插⼊ [SEP]  标记，表示分句符号，⽤\\n于断开输⼊语料中的两个句⼦。\\n1.\\n2.\\n3.\\n1.\\n2.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d1df3759-acd3-44d3-bf26-48efb9d31ea6', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '5'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 5\\n2.2 Embedding\\nERT 的输⼊的编码向量（⻓度是 512）是 3 个嵌⼊特征的单位和，这三个词嵌⼊特征\\n是：\\n位置嵌⼊（Position Embedding）：位置嵌⼊是指将单词的位置信息编码成特\\n征向量，位置嵌⼊是向模型中引⼊单词位置关系的⾄关重要的⼀环；\\nWordPiece 嵌⼊：WordPiece 是指将单词划分成⼀组有限的公共⼦词单元，能\\n在单词的有效性和字符的灵活性之间取得⼀个折中的平衡。例如上图的示例\\n中‘playing’被拆分成了‘play’和‘ing’；\\n分割嵌⼊（Segment Embedding）：⽤于区分两个句⼦，例如 B 是否是 A 的下\\n⽂（对话场景，问答场景等）。对于句⼦对，第⼀个句⼦的特征值是 0，第⼆个\\n句⼦的特征值是 1。」\\n3.模型细节\\n3.1 BERT 在第⼀句前会加⼀个[CLS]标志\\nBERT 在第⼀句前会加⼀个[CLS]标志，最后⼀层该位对应向量可以作为整句话的语\\n义表示，从⽽⽤于下游的分类任务等。\\n3.2 BERT 的三个 Embedding 直接相加会对语义有影响吗\\nBERT 的三个 Embedding 相加，本质可以看作⼀个特征的融合，强⼤如 BERT 应该\\n可以学到融合后特征的语义信息的。\\nEmbedding 的本质：Embedding 层就是以 one hot 为输⼊、中间层节点为字向量维\\n数的全连接层！⽽这个全连接层的参数，就是⼀个“字向量表”！\\n从运算上来看，one hot 型的矩阵相乘，就像是相当于查表，于是它直接⽤查表作为\\n操作，⽽不写成矩阵再运算，这⼤⼤降低了运算量。再次强调，降低了运算量不是因\\n为词向量的出现，⽽是因为把 one hot 型的矩阵运算简化为了查表操作。\\n在这⾥想⽤⼀个例⼦再尝试解释⼀下：\\n假设 token Embedding 矩阵维度是 [4,768] ；position Embedding 矩阵维\\n度是 [3,768] ；segment Embedding 矩阵维度是 [2,768] 。\\n1.\\n2.\\n3.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a92175f9-1c91-44a0-805a-89a88c6205cc', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '6'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 6\\n对于⼀个字，假设它的 token one-hot 是 [1,0,0,0] ；它的 position one-\\nhot 是 [1,0,0] ；它的 segment one-hot 是 [1,0] 。\\n那这个字最后的 word Embedding，就是上⾯三种 Embedding 的加和。\\n如此得到的 word Embedding，和 concat 后的特征：\\n[1,0,0,0,1,0,0,1,0] ，再过维度为 [4+3+2,768] = [9, 768]  的全连\\n接层，得到的向量其实就是⼀样的。\\n1.4 使⽤ BERT 预训练模型为什么最多只能输⼊ 512 个词?\\n这是 Google BERT 预训练模型初始设置的原因，前者对应 Position Embeddings，\\n后者对应 Segment Embeddings\\nBERT 输⼊：\\ntoken embedding ：词向量表示 ，该向量既可以随机初始化，也可以利⽤\\nWord2Vector 等算法进⾏预训练以作为初始值，使⽤ WordPiece tokenization\\n让 BERT 在处理英⽂⽂本的时候仅需要存储 30,522 个词，⽽且很少遇到 oov 的\\n词，token embedding 是必须的；\\nposition embedding ：和 Transformer 的 sin、cos 函数编码不同，直接去\\n训练了⼀个 position embedding。给每个位置词⼀个随机初始化的词向量，再训\\n练；\\n segment embedding ：该向量的取值在模型训练过程中⾃动学习，⽤于刻画\\n⽂本的全局语义信息，并与单字/词的语义信息相融合。\\n输出是⽂本中各个字/词融合了全⽂语义信息后的向量表示。\\n3.3 BERT 如何区分⼀词多义？\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c3f33784-e7cb-4c7d-a681-30d40138f48f', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '7'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 7\\n同⼀个字在转换为 bert 的输⼊之后（id），embedding 的向量是⼀样，但是通过\\nbert 中的多层 transformer encoder 之后，attention 关注不同的上下⽂，就会导致\\n不同句⼦输⼊到 bert 之后，相同字输出的字向量是不同的，这样就解决了⼀词多义\\n的问题。\\n3.4  BERT 中 Normalization 结构：LayerNorm\\n采⽤ LayerNorm 结构，和 BatchNorm 的区别主要是做规范化的维度不同。\\nBatchNorm 针对⼀个 batch ⾥⾯的数据进⾏规范化，Batch Normalization 是\\n对这批样本的同⼀维度特征做归⼀化\\nLayerNorm 则是针对单个样本，不依赖于其他数据，常被⽤于⼩ mini-batch 场\\n景、动态⽹络场景和 RNN。Layer Normalization 是对这单个样本的所有维度特\\n征做归⼀化。\\nBatchNorm 的缺点：\\n需要较⼤的 batch 以体现整体数据分布\\n训练阶段需要保存每个 batch 的均值和⽅差，以求出整体均值和⽅差在 infrence\\n阶段使⽤\\n不适⽤于可变⻓序列的训练，如 RNN\\nLayer Normalization：⼀个独⽴于 batch size 的算法，所以⽆论⼀个 batch 样本数\\n多少都不会影响参与 LN 计算的数据量，从⽽解决 BN 的两个问题。LN 的做法是根\\n据样本的特征数做归⼀化。Layer Normalization 不依赖于 batch 的⼤⼩和输⼊\\nsequence 的深度，因此可以⽤于 batch-size 为 1 和 RNN 中对边⻓的输⼊\\nsequence 的 normalize 操作。但在⼤批量的样本训练时，效果没 BN 好。\\n实践证明，LN ⽤于 RNN 进⾏ Normalization 时，取得了⽐ BN 更好的效果。但⽤于\\nCNN 时，效果并不如 BN 明显。\\n3.5  为什么说 ELMO 是伪双向，BERT 是真双向？\\nELMo 是伪双向，只是将左到右，右到左的信息加起来，⽽且⽤的是 lstm，同时\\n缺点也是显⽽易⻅的，模型参数太多，⽽且模型太⼤，少量数据训练时，容易过\\n拟合。\\n1.\\n2.\\n3.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f6c5999-315b-43b8-929d-3fa61632a9bc', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '8'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 8\\nBERT 的预训练模型中，预训练任务是⼀个 mask LM ，通过随机的把句⼦中的\\n单词替换成 mask 标签， 然后对单词进⾏预测。\\n3.6  BERT 和 Transformer Encoder 的差异有哪些？\\n与 Transformer 本身的 Encoder 端相⽐，BERT 的 Transformer Encoder 端输⼊的\\n向量表示，多了 Segment Embeddings 。 \\n加⼊ Segment Embeddings 的原因：Bert 会处理句对分类、问答等任务，这⾥会\\n出现句对关系，⽽两个句⼦是有先后顺序关系的，如果不考虑，就会出现词袋⼦之类\\n的问题（如：武松打⻁ 和 ⻁打武松 是⼀个意思了 ~），因此 Bert 加⼊了句⼦向量。\\n3.7 Scaled Dot Product:为什么是缩放点积，⽽不是点积模型？\\n当输⼊信息的维度 d ⽐较⾼，点积模型的值通常有⽐较⼤⽅差，从⽽导致 softmax\\n函数的梯度会⽐较⼩。因此，缩放点积模型可以较好地解决这⼀问题。\\n常⽤的 Attention 机制为加性模型和点积模型，理论上加性模型和点积模型的复杂度\\n差不多，但是点积模型在实现上可以更好地利⽤矩阵乘积，从⽽计算效率更⾼（实际\\n上，随着维度 d 的增⼤，加性模型会明显好于点积模型）。\\n3.8 FFN 的作⽤？\\n增强模型的特征提取能⼒\\nFFN 中的 ReLU 成为了⼀个主要的提供⾮线性变换的单元。\\n3.9 BERT ⾮线性的来源\\n前馈层的 GeLU 激活函数\\nself-attention：self-attention 是⾮线性的（来⾃ softmax）\\n3.10 MLM 任务，对于在数据中随机选择 15% 的标记，其中\\n80% 被换位[mask]，10% 不变、10% 随机替换其他单词，原因\\nGeLU：在激活中引⼊了随机正则的思想，根据当前 input ⼤于其余 inputs 的概率\\n进⾏随机正则化，即为在 mask 时依赖输⼊的数据分布，即 x 越⼩越有可能被\\nmask 掉，因此服从伯努利分布\\n ，其中，\\n \\nReLU：缺乏随机因素，只⽤ 0 和 1\\nBernoulli(ϕ(x))\\nϕ(x) = P(X ≤ x)\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a54c473d-9afa-4b3a-b11f-a4bb290c5d29', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '9'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 9\\n是什么？\\n典型的 Denosing Autoencoder 的思路，那些被 Mask 掉的单词就是在输⼊侧加⼊的\\n所谓噪⾳。类似 BERT 这种预训练模式，被称为 DAE LM。因此总结来说 BERT 模\\n型 [Mask]  标记就是引⼊噪⾳的⼿段。\\n预测⼀个词汇时，模型并不知道输⼊对应位置的词汇是否为正确的词汇（ 10% 概\\n率），这就迫使模型更多地依赖于上下⽂信息去预测词汇，并且赋予了模型⼀定的纠\\n错能⼒。\\n两个缺点：  \\n因为 Bert ⽤于下游任务微调时， [MASK] 标记不会出现，它只出现在预训练任\\n务中。这就造成了预训练和微调之间的不匹配，微调不出现 [MASK] 这个标记，\\n模型好像就没有了着⼒点、不知从哪⼊⼿。所以只将 80% 的替换为 [mask] ，\\n但这也只是缓解、不能解决。  \\n相较于传统语⾔模型，Bert 的每批次训练数据中只有 15% 的标记被预测，这导\\n致模型需要更多的训练步骤来收敛。\\n3.11 其 mask 相对于 CBOW 有什么异同点？\\n相同点：\\nCBOW 的核⼼思想是：给定上下⽂，根据它的上⽂ Context-Before 和下⽂ \\nContext-after 去预测 input word。  \\n⽽ BERT 本质上也是这么做的，但是 BERT 的做法是给定⼀个句⼦，会随机\\nMask 15% 的词，然后让 BERT 来预测这些 Mask 的词。\\n不同点：\\n在 CBOW 中，每个单词都会成为 input word，⽽ BERT 不是这么做的，原因是\\n这样做的话，训练数据就太⼤了，⽽且训练时间也会⾮常⻓。\\n对于输⼊数据部分，CBOW 中的输⼊数据只有待预测单词的上下⽂，⽽ BERT\\n的输⼊是带有 [MASK]  token 的“完整”句⼦，也就是说 BERT 在输⼊端将待预\\n测的 input word ⽤ [MASK]  token 代替了。\\n通过 CBOW 模型训练后，每个单词的 word embedding 是唯⼀的，因此并不能\\n很好的处理⼀词多义的问题，⽽ BERT 模型得到的 word embedding(token \\n1.\\n2.\\n1.\\n2.\\n3.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1acb2603-3675-4013-a1ef-539e09425a62', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '10'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 10\\nembedding)融合了上下⽂的信息，就算是同⼀个单词，在不同的上下⽂环境\\n下，得到的 word embedding 是不⼀样的。\\n3.12 对于⻓度较⻓的语料，如何训练？\\n对于⻓⽂本，有两种处理⽅式，截断和切分。\\n切分: 将⽂本分成 k 段，每段的输⼊和 Bert 常规输⼊相同，第⼀个字符是[CLS]\\n表示这段的加权信息。⽂中使⽤了 Max-pooling, Average pooling 和 self-\\nattention 结合这些⽚段的表示。\\n4.BERT 损失函数\\nBert 损失函数组成：第⼀部分是来⾃ Mask-LM 的单词级别分类任务；另⼀部分是\\n句⼦级别的分类任务；\\n优点：通过这两个任务的联合学习，可以使得 BERT 学习到的表征既有 token 级别\\n信息，同时也包含了句⼦级别的语义信息。\\n : BERT 中 Encoder 部分的参数；\\n : 是 Mask-LM 任务中在 Encoder 上所接的输出层中的参数；\\n  :是句⼦预测任务中在 Encoder 接上的分类器参数；\\n在第⼀部分的损失函数中，如果被 mask 的词集合为 M，因为它是⼀个词典⼤⼩ |V| \\n上的多分类问题，所⽤的损失函数叫做负对数似然函数（且是最⼩化，等价于最⼤化\\n对数似然函数），那么具体说来有：\\n截断：⼀般来说⽂本中最重要的信息是开始和结尾，因此⽂中对于⻓⽂本做了截\\n断处理。\\nhead-only：保留前 510 个字符\\ntail-only：保留后 510 个字符\\nhead+tail：保留前 128 个和后 382 个字符\\n1.\\n2.\\n3.\\nL θ, θ\\n , θ\\n =\\n(\\n1\\n2)\\nL\\n θ, θ\\n +\\n1 (\\n1)\\nL\\n θ, θ\\n \\n2 (\\n2)\\nθ\\nθ\\n 1\\nθ\\n 2\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c16d5ccb-4c18-45d0-982a-2955b50c0ffc', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '11'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 11\\n在第⼆部分的损失函数中，在句⼦预测任务中，也是⼀个分类问题的损失函数：\\n5.模型优缺点和局限性\\n5.1 BERT 优点\\nTransformer Encoder 因为有 Self-attention 机制，因此 BERT ⾃带双向功能  \\n计算可并⾏化  \\n微调成本⼩  \\n因为双向功能以及多层 Self-attention 机制的影响，使得 BERT 必须使⽤ Cloze\\n版的语⾔模型 Masked-LM 来完成 token 级别的预训练  \\n为了获取⽐词更⾼级别的句⼦级别的语义表征，BERT 加⼊了 Next Sentence \\nPrediction 来和 Masked-LM ⼀起做联合训练  \\n为了适配多任务下的迁移学习，BERT 设计了更通⽤的输⼊层和输出层\\n5.2 BERT 缺点\\n[MASK] 标记在实际预测中不会出现，训练时⽤过多 [MASK] 影响模型表现  \\n每个 batch 只有 15% 的 token 被预测，所以 BERT 收敛得⽐ left-to-right 模型\\n要慢（它们会预测每个 token）  \\ntask1 的随机遮挡策略略显粗犷，推荐阅读《Data Nosing As Smoothing In \\nNeural Network Language Models》  \\nBERT 对硬件资源的消耗巨⼤（⼤模型需要 16 个 tpu，历时四天；更⼤的模型需\\n要 64 个 tpu，历时四天。\\nL\\n θ, θ\\n =\\n1 (\\n1)\\n−\\n log p m = m\\n ∣ θ, θ\\n , m\\n ∈\\ni=1\\n∑\\nM\\n(\\ni\\n1)\\ni\\n[1, 2, … , ∣V ∣]\\nL\\n θ, θ\\n =\\n2 (\\n2)\\n−\\n log p n = n\\n ∣ θ, θ\\n , n\\n ∈\\nj=1\\n∑\\nN\\n(\\ni\\n2)\\ni\\n[IsNext, NotNext]\\n1.\\n2.\\n3.\\n4.\\n5.\\n6.\\n1.\\n2.\\n3.\\n4.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3245f220-5044-486c-a5ac-e9943355cc26', embedding=None, metadata={'total_pages': 12, 'file_path': '../AIInterviewer/docs/llm_interview_note/pdf_note/bert细节.pdf', 'source': '12'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='bert细节 - 12\\n5.3 BERT 局限性\\n从 XLNet 论⽂中，提到了 BERT 的两个缺点，分别如下\\n被 mask 掉的单词之间是有关系的，⽐如”New York is a \\ncity”，”New”和”York”两个词，那么给定”is a city”的条件\\n下”New”和”York”并不独⽴，因为”New York”是⼀个实体，看到”New”则后⾯\\n出现”York”的概率要⽐看到”Old”后⾯出现”York”概率要⼤得多。\\n但是需要注意的是，这个问题并不是什么⼤问题，甚⾄可以说对最后的结果并没\\n有多⼤的影响，因为本身 BERT 预训练的语料就是海量的(动辄⼏⼗个 G)，所以\\n如果训练数据⾜够⼤，其实不靠当前这个例⼦，靠其它例⼦，也能弥补被 Mask\\n单词直接的相互关系问题，因为总有其它例⼦能够学会这些单词的相互依赖关\\n系。\\nBERT 的在预训练时会出现特殊的 [MASK] ，但是它在下游的 fine-tune 中不会\\n出现，这就出现了预训练阶段和 fine-tune 阶段不⼀致的问题。其实这个问题对\\n最后结果产⽣多⼤的影响也是不够明确的，因为后续有许多 BERT 相关的预训练\\n模型仍然保持了 [MASK] 标记，也取得了很⼤的结果，⽽且很多数据集上的结果\\n也⽐ BERT 要好。但是确确实实引⼊ [MASK] 标记，也是为了构造⾃编码语⾔模\\n型⽽采⽤的⼀种折中⽅式。\\nBERT 在分词后做 [MASK] 会产⽣的⼀个问题，为了解决 OOV 的问题，通常会\\n把⼀个词切分成更细粒度的 WordPiece。BERT 在 Pretraining 的时候是随机\\nMask 这些 WordPiece 的，这就可能出现只 Mask ⼀个词的⼀部分的情况，这样\\n它只需要记住⼀些词(WordPiece 的序列)就可以完成这个任务，⽽不是根据上下\\n⽂的语义关系来预测出来的。类似的中⽂的词”模型”也可能被 Mask 部分(其实\\n⽤”琵琶”的例⼦可能更好，因为这两个字只能⼀起出现⽽不能单独出现)，这也会\\n让预测变得容易。为了解决这个问题，很⾃然的想法就是词作为⼀个整体要么都\\nMask 要么都不 Mask，这就是所谓的 Whole Word Masking。这是⼀个很简单\\n的想法，对于 BERT 的代码修改也⾮常少，只是修改⼀些 Mask 的那段代码。\\n1.\\n2.\\n3.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(test_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# 1.llm概念',\n",
       " '\\\\[toc]',\n",
       " '### 1.目前 主流的开源模型体系 有哪些？',\n",
       " '目前主流的开源LLM（语言模型）模型体系包括以下几个：',\n",
       " '1. **GPT（Generative Pre-trained Transformer）系列**：由OpenAI发布的一系列基于Transformer架构的语言模型，包括GPT、GPT-2、GPT-3等。GPT模型通过在大规模无标签文本上进行预训练，然后在特定任务上进行微调，具有很强的生成能力和语言理解能力。',\n",
       " '2. **BERT（Bidirectional Encoder Representations from Transformers）**：由Google发布的一种基于Transformer架构的双向预训练语言模型。BERT模型通过在大规模无标签文本上进行预训练，然后在下游任务上进行微调，具有强大的语言理解能力和表征能力。',\n",
       " '3. **XLNet**：由CMU和Google Brain发布的一种基于Transformer架构的自回归预训练语言模型。XLNet模型通过自回归方式预训练，可以建模全局依赖关系，具有更好的语言建模能力和生成能力。',\n",
       " '4. **RoBERTa**：由Facebook发布的一种基于Transformer架构的预训练语言模型。RoBERTa模型在BERT的基础上进行了改进，通过更大规模的数据和更长的训练时间，取得了更好的性能。',\n",
       " '5. **T5（Text-to-Text Transfer Transformer）**：由Google发布的一种基于Transformer架构的多任务预训练语言模型。T5模型通过在大规模数据集上进行预训练，可以用于多种自然语言处理任务，如文本分类、机器翻译、问答等。',\n",
       " '这些模型在自然语言处理领域取得了显著的成果，并被广泛应用于各种任务和应用中。',\n",
       " '### 2.prefix LM 和 causal LM 区别是什么？',\n",
       " 'Prefix LM（前缀语言模型）和Causal LM（因果语言模型）是两种不同类型的语言模型，它们的区别在于生成文本的方式和训练目标。',\n",
       " '#### 2.1 Prefix LM',\n",
       " 'Prefix LM其实是Encoder-Decoder模型的变体，为什么这样说？解释如下：',\n",
       " '1. 在标准的Encoder-Decoder模型中，Encoder和Decoder各自使用一个独立的Transformer',\n",
       " '2. 而在Prefix LM，Encoder和Decoder则共享了同一个Transformer结构，在Transformer内部通过Attention Mask机制来实现。',\n",
       " '与标准Encoder-Decoder类似，**Prefix LM在Encoder部分采用Auto Encoding (AE-自编码)模式，即前缀序列中任意两个token都相互可见，而Decoder部分采用Auto Regressive  (AR-自回归)模式，即待生成的token可以看到Encoder侧所有token(包括上下文)和Decoder侧已经生成的token，但不能看未来尚未产生的token**。',\n",
       " '下面的图很形象地解释了Prefix LM的Attention Mask机制(左)及流转过程(右)。',\n",
       " 'Prefix LM的代表模型有UniLM、GLM',\n",
       " '#### 2.2 Causal LM',\n",
       " 'Causal LM是因果语言模型，目前流行地大多数模型都是这种结构，别无他因，因为GPT系列模型内部结构就是它，还有开源界的LLaMa也是。',\n",
       " 'Causal LM只涉及到Encoder-Decoder中的Decoder部分，采用Auto Regressive模式，直白地说，就是**根据历史的token来预测下一个token，也是在Attention Mask这里做的手脚**。',\n",
       " '参照着Prefix LM，可以看下Causal LM的Attention Mask机制(左)及流转过程(右)。',\n",
       " '![](image/image_ZPQiHay1ZD.png)',\n",
       " '#### 2.3 总结',\n",
       " '1. **Prefix LM**：前缀语言模型是一种生成模型，它在生成每个词时都可以考虑之前的上下文信息。在生成时，前缀语言模型会根据给定的前缀（即部分文本序列）预测下一个可能的词。这种模型可以用于文本生成、机器翻译等任务。',\n",
       " '2. **Causal LM**：因果语言模型是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种模型可以用于文本生成、语言建模等任务。',\n",
       " '总结来说，前缀语言模型可以根据给定的前缀生成后续的文本，而因果语言模型只能根据之前的文本生成后续的文本。它们的训练目标和生成方式略有不同，适用于不同的任务和应用场景。',\n",
       " '### 3.大模型LLM的 训练目标',\n",
       " '大型语言模型（Large Language Models，LLM）的训练目标通常是**最大似然估计（Maximum Likelihood Estimation，MLE）**。最大似然估计是一种统计方法，用于从给定数据中估计概率模型的参数。',\n",
       " '在LLM的训练过程中，使用的数据通常是大量的文本语料库。训练目标是**最大化模型生成训练数据中观察到的文本序列的概率**。具体来说，对于每个文本序列，模型根据前面的上下文生成下一个词的条件概率分布，并通过最大化生成的词序列的概率来优化模型参数。',\n",
       " '为了最大化似然函数，可以使用梯度下降等优化算法来更新模型参数，使得模型生成的文本序列的概率逐步提高。在训练过程中，通常会使用批量训练（batch training）的方法，通过每次处理一小批数据样本来进行参数更新。',\n",
       " '### 4.涌现能力是啥原因？',\n",
       " '[大语言模型的涌现能力：现象与解释 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/621438653 \"大语言模型的涌现能力：现象与解释 - 知乎 (zhihu.com)\")',\n",
       " '涌现能力（Emergent Ability）是指**模型在训练过程中能够生成出令人惊喜、创造性和新颖的内容或行为**。这种能力使得模型能够超出其训练数据所提供的内容，并产生出具有创造性和独特性的输出。',\n",
       " '涌现能力的产生可以归因于以下几个原因：',\n",
       " '1. **任务的评价指标不够平滑**：因为很多任务的评价指标不够平滑，导致我们现在看到的涌现现象。如果评价指标要求很严格，要求一字不错才算对，那么Emoji\\\\_movie任务我们就会看到涌现现象的出现。但是，如果我们把问题形式换成多选题，就是给出几个候选答案，让LLM选，那么随着模型不断增大，任务效果在持续稳定变好，但涌现现象消失，如上图图右所示。这说明评价指标不够平滑，起码是一部分任务看到涌现现象的原因。',\n",
       " '2. **复杂任务** **vs** **子任务**：展现出涌现现象的任务有一个共性，就是任务往往是由多个子任务构成的复杂任务。也就是说，最终任务过于复杂，如果仔细分析，可以看出它由多个子任务构成，这时候，子任务效果往往随着模型增大，符合 Scaling Law，而最终任务则体现为涌现现象。',\n",
       " '3. **用** **Grokking** （顿悟）**来解释涌现**：对于某个任务T，尽管我们看到的预训练数据总量是巨大的，但是与T相关的训练数据其实数量很少。当我们推大模型规模的时候，往往会伴随着增加预训练数据的数据量操作，这样，当模型规模达到某个点的时候，与任务T相关的数据量，突然就达到了最小要求临界点，于是我们就看到了这个任务产生了Grokking现象。',\n",
       " '尽管涌现能力为模型带来了创造性和独特性，但也需要注意其生成的内容可能存在偏差、错误或不完整性。因此，在应用和使用涌现能力强的模型时，需要谨慎评估和验证生成的输出，以确保其质量和准确性。',\n",
       " '### 5.为何现在的大模型大部分是Decoder only结构',\n",
       " '1. **Encoder的低秩问题**：Encoder的双向注意力会存在低秩问题，这可能会削弱模型表达能力，就生成任务而言，引入双向注意力并无实质好处。',\n",
       " '2. **更好的Zero-Shot性能、更适合于大语料自监督学习**：decoder-only 模型在没有任何 tuning 数据的情况下、zero-shot 表现最好，而 encoder-decoder 则需要在一定量的标注数据上做 multitask finetuning 才能激发最佳性能。',\n",
       " '3. **效率问题**：decoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个Token的表示之和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到。',\n",
       " '### 6.大模型架构介绍',\n",
       " 'Transformer 模型一开始是用来做 seq2seq 任务的，所以它包含 Encoder 和 Decoder 两个部分；他们两者的区别主要是，**Encoder 在抽取序列中某一个词的特征时能够看到整个序列中所有的信息，即上文和下文同时看到**；而 **Decoder 中因为有 mask 机制的存在，使得它在编码某一个词的特征时只能看到自身和它之前的文本信息**。',\n",
       " '首先概述几种主要的架构:&#x20;',\n",
       " '- 以BERT为代表的**encoder-only**',\n",
       " '- 以T5和BART为代表的**encoder-decoder**',\n",
       " '- 以GPT为代表的**decoder-only**，',\n",
       " '- 以UNILM9为代表的PrefixLM(相比于GPT只改了attention mask，前缀部分是双向，后面要生成的部分是单向的causal mask%)&#x20;',\n",
       " '![](image/image_FTjn7ZU5Xf.png)',\n",
       " '### 6.LLMs复读机问题',\n",
       " '#### 6.1 什么是 LLMs 复读机问题？',\n",
       " 'LLMs复读机问题（LLMs Parroting Problem）是指大型语言模型在生成文本时过度依赖输入文本的复制，而缺乏创造性和独特性。当面对一个问题或指令时，模型可能会简单地复制输入文本的一部分或全部内容，并将其作为生成的输出，而不是提供有意义或新颖的回应。',\n",
       " '#### 6.2 为什么会出现 LLMs 复读机问题？',\n",
       " '1. **数据偏差**：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。',\n",
       " '2. **训练目标的限制**：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现。',\n",
       " '3. **缺乏多样性的训练数据**：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性，导致复读机问题的出现。',\n",
       " '4. **模型结构和参数设置**：大型语言模型的结构和参数设置也可能对复读机问题产生影响。例如，模型的注意力机制和生成策略可能导致模型更倾向于复制输入的文本。',\n",
       " '#### 6.3 如何缓解 LLMs 复读机问题？',\n",
       " '为了缓解LLMs复读机问题，可以尝试以下方法：',\n",
       " '1. **多样性训练数据**：在训练阶段，使用多样性的语料库来训练模型，避免数据偏差和重复文本的问题。这可以包括从不同领域、不同来源和不同风格的文本中获取数据。',\n",
       " '2. **引入噪声**：在生成文本时，引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。这可以通过在生成过程中对模型的输出进行采样或添加随机性来实现。',\n",
       " '3. **温度参数调整**：温度参数是用来控制生成文本的多样性的一个参数。通过调整温度参数的值，可以控制生成文本的独创性和多样性。较高的温度值会增加随机性，从而减少复读机问题的出现。',\n",
       " '4. **Beam搜索调整**：在生成文本时，可以调整Beam搜索算法的参数。Beam搜索是一种常用的生成策略，它在生成过程中维护了一个候选序列的集合。通过调整Beam大小和搜索宽度，可以控制生成文本的多样性和创造性。',\n",
       " '5. **后处理和过滤**：对生成的文本进行后处理和过滤，去除重复的句子或短语，以提高生成文本的质量和多样性。可以使用文本相似度计算方法或规则来检测和去除重复的文本。',\n",
       " '6. **人工干预和控制**：对于关键任务或敏感场景，可以引入人工干预和控制机制，对生成的文本进行审查和筛选，确保生成结果的准确性和多样性。',\n",
       " '需要注意的是，缓解LLMs复读机问题是一个复杂的任务，没有一种通用的解决方案。不同的方法可能适用于不同的场景和任务，需要根据具体情况进行选择和调整。此外，解决复读机问题还需要综合考虑数据、训练目标、模型架构和生成策略等多个因素，需要进一步的研究和实践来提高大型语言模型的生成文本多样性和创造性。',\n",
       " '### 7.LLMs输入句子长度理论上可以无限长吗？',\n",
       " '**理论上来说，LLMs（大型语言模型）可以处理任意长度的输入句子，但实际上存在一些限制和挑战**。下面是一些相关的考虑因素：',\n",
       " '1. **计算资源**：生成长句子需要更多的计算资源，包括内存和计算时间。由于LLMs通常是基于神经网络的模型，计算长句子可能会导致内存不足或计算时间过长的问题。',\n",
       " '2. **模型训练和推理**：训练和推理长句子可能会面临一些挑战。在训练阶段，处理长句子可能会导致梯度消失或梯度爆炸的问题，影响模型的收敛性和训练效果。在推理阶段，生成长句子可能会增加模型的错误率和生成时间。',\n",
       " '3. **上下文建模**：LLMs是基于上下文建模的模型，长句子的上下文可能会更加复杂和深层。模型需要能够捕捉长句子中的语义和语法结构，以生成准确和连贯的文本。',\n",
       " '### 8.什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？',\n",
       " '选择使用哪种大模型，如Bert、LLaMA或ChatGLM，取决于具体的应用场景和需求。下面是一些指导原则：',\n",
       " '1. **Bert模型**：Bert是一种预训练的语言模型，**适用于各种自然语言处理任务**，如文本分类、命名实体识别、语义相似度计算等。如果你的任务是通用的文本处理任务，而不依赖于特定领域的知识或语言风格，Bert模型通常是一个不错的选择。Bert由一个Transformer编码器组成，更适合于NLU相关的任务。',\n",
       " '2. **LLaMA模型**：LLaMA（Large Language Model Meta AI）包含从 7B 到 65B 的参数范围，训练使用多达14,000亿tokens语料，具有常识推理、问答、数学推理、代码生成、语言理解等能力。LLaMA由一个Transformer解码器组成。训练预料主要为以英语为主的拉丁语系，不包含中日韩文。所以适合于英文文本生成的任务。',\n",
       " '3. **ChatGLM模型**：ChatGLM是一个面向对话生成的语言模型，适用于构建聊天机器人、智能客服等对话系统。如果你的应用场景需要模型能够生成连贯、流畅的对话回复，并且需要处理对话上下文、生成多轮对话等，ChatGLM模型可能是一个较好的选择。ChatGLM的架构为Prefix decoder，训练语料为中英双语，中英文比例为1:1。所以适合于中文和英文文本生成的任务。',\n",
       " '在选择模型时，还需要考虑以下因素：',\n",
       " '- 数据可用性：不同模型可能需要不同类型和规模的数据进行训练。确保你有足够的数据来训练和微调所选择的模型。',\n",
       " '- 计算资源：大模型通常需要更多的计算资源和存储空间。确保你有足够的硬件资源来支持所选择的模型的训练和推理。',\n",
       " '- 预训练和微调：大模型通常需要进行预训练和微调才能适应特定任务和领域。了解所选择模型的预训练和微调过程，并确保你有相应的数据和时间来完成这些步骤。',\n",
       " '最佳选择取决于具体的应用需求和限制条件。在做出决策之前，建议先进行一些实验和评估，以确定哪种模型最适合你的应用场景。',\n",
       " '### 9.各个专业领域是否需要各自的大模型来服务？',\n",
       " '各个专业领域通常需要各自的大模型来服务，原因如下：',\n",
       " '1. **领域特定知识**：不同领域拥有各自特定的知识和术语，需要针对该领域进行训练的大模型才能更好地理解和处理相关文本。例如，在医学领域，需要训练具有医学知识的大模型，以更准确地理解和生成医学文本。',\n",
       " '2. **语言风格和惯用语**：各个领域通常有自己独特的语言风格和惯用语，这些特点对于模型的训练和生成都很重要。专门针对某个领域进行训练的大模型可以更好地掌握该领域的语言特点，生成更符合该领域要求的文本。',\n",
       " '3. **领域需求的差异**：不同领域对于文本处理的需求也有所差异。例如，金融领域可能更关注数字和统计数据的处理，而法律领域可能更关注法律条款和案例的解析。因此，为了更好地满足不同领域的需求，需要专门针对各个领域进行训练的大模型。',\n",
       " '4. **数据稀缺性**：某些领域的数据可能相对较少，无法充分训练通用的大模型。针对特定领域进行训练的大模型可以更好地利用该领域的数据，提高模型的性能和效果。',\n",
       " '尽管需要各自的大模型来服务不同领域，但也可以共享一些通用的模型和技术。例如，通用的大模型可以用于处理通用的文本任务，而领域特定的模型可以在通用模型的基础上进行微调和定制，以适应特定领域的需求。这样可以在满足领域需求的同时，减少模型的重复训练和资源消耗。',\n",
       " '### 10.如何让大模型处理更长的文本？',\n",
       " '要让大模型处理更长的文本，可以考虑以下几个方法：',\n",
       " '1. **分块处理**：将长文本分割成较短的片段，然后逐个片段输入模型进行处理。这样可以避免长文本对模型内存和计算资源的压力。在处理分块文本时，可以使用重叠的方式，即将相邻片段的一部分重叠，以保持上下文的连贯性。',\n",
       " '2. **层次建模**：通过引入层次结构，将长文本划分为更小的单元。例如，可以将文本分为段落、句子或子句等层次，然后逐层输入模型进行处理。这样可以减少每个单元的长度，提高模型处理长文本的能力。',\n",
       " '3. **部分生成**：如果只需要模型生成文本的一部分，而不是整个文本，可以只输入部分文本作为上下文，然后让模型生成所需的部分。例如，输入前一部分文本，让模型生成后续的内容。',\n",
       " '4. **注意力机制**：注意力机制可以帮助模型关注输入中的重要部分，可以用于处理长文本时的上下文建模。通过引入注意力机制，模型可以更好地捕捉长文本中的关键信息。',\n",
       " '5. **模型结构优化**：通过优化模型结构和参数设置，可以提高模型处理长文本的能力。例如，可以增加模型的层数或参数量，以增加模型的表达能力。还可以使用更高效的模型架构，如Transformer等，以提高长文本的处理效率。',\n",
       " '需要注意的是，处理长文本时还需考虑计算资源和时间的限制。较长的文本可能需要更多的内存和计算时间，因此在实际应用中需要根据具体情况进行权衡和调整。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
